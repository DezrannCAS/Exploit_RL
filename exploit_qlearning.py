import numpy as np
import networkx as nx
from scipy.sparse.csgraph import connected_components

class ResourcePool:
    def __init__(self, initial_stock, max_stock, growth_rate):
        self.stock = initial_stock
        self.max_stock = max_stock
        self.growth_rate = growth_rate

    def reset(self, initial_stock):
        self.stock = initial_stock

    def step(self, dt, strategy):
        s0 = self.stock
        smax = self.max_stock
        g = self.growth_rate
        E = g * 0.5 * (3 - 2 * strategy)
        b = g - E
        gs0 = g * s0
        s1 = s0 * smax * b / ((smax * b - gs0) * np.exp(-b * dt) + gs0)
        harvest = E * s1
        self.stock = s1

        return harvest

class Agent:
    def __init__(self, strategy, alpha, gamma, epsilon, resource_pool):
        self.strategy = strategy
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.resource_pool = resource_pool
        self.q_table = np.zeros((2, 2))  # Q-table with states sustainable effort = [0, 1] and actions imitate = [0, 1]

    def choose_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.choice([0, 1])  # explore
        return np.argmax(self.q_table[state])  # exploit

    def perform_action(self, dt):
        return self.resource_pool.step(dt, self.strategy)
        
    def update_q_table(self, state, action, reward, next_state):
        best_next_action = np.argmax(self.q_table[next_state])
        td_target = reward + self.gamma * self.q_table[next_state][best_next_action]
        td_error = td_target - self.q_table[state][action]
        self.q_table[state][action] += self.alpha * td_error

class MultiAgentTrainer:
    def __init__(self, adjacency_matrix, population):
        self.num_agents = adjacency_matrix.shape[0]
        self.adjacency_matrix = adjacency_matrix
        self.population = population
        self.consensus = self.check_for_consensus()
        self.current_time = 0

    def check_for_consensus(self):
        cc = connected_components(self.adjacency_matrix, directed=False)[1]
        return all(len(np.unique([self.population[i].strategy for i in (cc == c).nonzero()[0]])) == 1 for c in np.unique(cc))

    def give_states(self, state_case="majority"):
        """
        Give the state of all agents: either the strategy of a randomly picked neighbor (discrete state space), 
        or whether the majority of the neighborhood is strategy 0 or 1 (discrete state space),
        or the fraction of sustainable strategies (continuous state space)
        """
        states_dict = {}

        if state_case == "random pick":
            for i in range(len(self.population)):
                neighbors_indices = self.adjacency_matrix[i].nonzero()[0]
                assert(len(neighbors_indices) > 0, 'No neighbor found')
                neighbor_idx = np.random.choice(neighbors_indices)
                neighbor_strat = self.population[neighbor_idx].strategy
                states_dict[i] = (neighbor_strat, neighbor_idx)
            return states_dict
        
        elif state_case == "majority":
            for i in range(len(self.population)):
                neighbors_indices = self.adjacency_matrix[i].nonzero()[0]
                assert(len(neighbors_indices) > 0, 'No neighbor found')
                neighbors_strategies = [self.population[i].strategy for i in neighbors_indices]
                majority_strategy = 0 if neighbors_strategies.count(0) > neighbors_strategies.count(1) else 1 
                states_dict[i] = (majority_strategy, neighbors_indices)
            return states_dict

        elif state_case == "fraction":
            for i in range(len(self.population)):
                neighbors_indices = self.adjacency_matrix[i].nonzero()[0]
                assert(len(neighbors_indices) > 0, 'No neighbor found')
                neighbors_strategies = [self.population[i].strategy for i in neighbors_indices]
                fraction_of_1 = neighbors_strategies.count(1) / len(neighbors_strategies)
                states_dict[i] = (fraction_of_1, neighbors_indices)
            return states_dict

        else:
            raise ValueError('The case for state formulation can either be "random pick", "majority" or "fraction".')

    def run(self, dt=1, num_steps=1000000000):
        states = self.give_states()

        for _ in range(num_steps):
            # Perform actions
            harvests = []
            for A in self.population:
                harvest = A.perform_action(dt)
                harvests.append(harvest)
            self.current_time += dt

            # Update strategies
            actions = []
            for i, agent in enumerate(self.population):
                majority_strategy = states[i][0]
                action = agent.choose_action(majority_strategy)
                if action == 1:  # imitate
                    agent.strategy = majority_strategy
                actions.append(action)
            
            # Update Q-tables
            next_states = self.give_states()
            for i, agent in enumerate(self.population):
                neighbors_indices = states[i][1]
                neighbors_harvests = np.mean([harvests[idx] for idx in neighbors_indices])
                dH = neighbors_harvests - harvests[i]

                agent.update_q_table(states[i][0], actions[i], -dH, next_states[i][0])

            states = next_states

            # Check consensus
            self.consensus = self.check_for_consensus()
            if self.consensus:
                return 1  # end with consensus

        return 0  # end without consensus

    def get_strategies(self):
        return np.array([agent.strategy for agent in self.population])

    def get_stocks(self):
        return np.array([agent.resource_pool.stock for agent in self.population])


#### Parameters ####

num_agents = 50
initial_stock = 1.0
max_stock = 1.0
growth_rate = 1.0
alpha = 0.1
gamma = 0.99
epsilon = 0.1
link_density = 0.35

#### Main code ####

population = [
    Agent(
        np.random.randint(2),
        alpha,
        gamma,
        epsilon,
        ResourcePool(initial_stock, max_stock, growth_rate)
    )
    for _ in range(num_agents)
]

adjacency_matrix = nx.adjacency_matrix(nx.erdos_renyi_graph(num_agents, link_density)).toarray()

trainer = MultiAgentTrainer(adjacency_matrix, population)
consensus = trainer.run()

print("Consensus:", bool(consensus))
print("--- Fraction of sustainable agents:", trainer.get_strategies().mean())
print("--- Model time:", trainer.current_time)
