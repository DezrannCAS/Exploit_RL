import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import random
import networkx as nx
from scipy.sparse.csgraph import connected_components
from scipy.stats import entropy

class ResourcePool:
    def __init__(self, x_range, y_range, s0_case, max_stock, growth_rate):
        self.x_range = x_range   # effort samples
        self.y_range = y_range   # state bins upper limits
        self.s0_case = s0_case
        if self.s0_case["case"] == "random":
            self.stock = random.random()
        elif self.s0_case["case"] == "constant":
            self.stock = self.s0_case["value"]
        self.max_stock = max_stock
        self.growth_rate = growth_rate

    def reset(self):
        if self.s0_case["case"] == "random":
            self.stock = random.random()
        elif self.s0_case["case"] == "constant":
            self.stock = self.s0_case["value"]

    def step(self, dt, E):
        s0 = self.stock
        smax = self.max_stock
        g = self.growth_rate
        b = g - E
        gs0 = g * s0
        s1 = s0 * smax * b / ((smax * b - gs0) * np.exp(-b * dt) + gs0)
        harvest = E * s1
        self.stock = s1

        return harvest
    
    def get_state_index(self):
        idx = np.nan
        for i, v in enumerate(self.y_range):
            if v >= self.stock:
                idx = i
                break
        return idx

class Agent:
    def __init__(self, x_range, y_range, alpha, gamma, epsilon, T, resource_pool):
        self.x_range = x_range
        self.y_range = y_range
        self.effort_idx = random.choice(range(len(self.x_range)))
        self.harvest = np.nan
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.T = T
        self.resource_pool = resource_pool
        self.q_table = np.zeros((len(self.y_range), len(self.x_range)))
      
    def reset(self):
        self.effort_idx = random.choice(range(len(self.x_range)))
        self.harvest = np.nan

    def get_effort_value(self):
        return self.x_range[self.effort_idx]

    def choose_action(self, state_idx, effort_tilde_idx):
        if np.random.rand() < self.epsilon:
            self.effort_idx = effort_tilde_idx
        else:
            self.effort_idx = np.argmax(self.q_table[state_idx])

    def perform_action(self, dt):
        harvest = self.resource_pool.step(dt, self.get_effort_value())
        self.harvest = harvest

    def update_q_table(self, state, action, reward, next_state):
        best_next_action = np.argmax(self.q_table[next_state])
        td_target = (1 - self.gamma) * reward + self.gamma * self.q_table[next_state][best_next_action]
        self.q_table[state][action] = (1 - self.alpha) * self.q_table[state][action] + self.alpha * td_target

class MultiAgentTrainer:
    def __init__(self, x_range, y_range, adjacency_matrix, population, num_episodes, num_steps):
        self.x_range = x_range
        self.y_range = y_range
        self.num_agents = adjacency_matrix.shape[0]
        self.adjacency_matrix = adjacency_matrix
        self.population = population
        self.current_time = 0
        self.num_steps = num_steps
        self.episode_stats = {'step': [], 'mean_effort': [], 'mean_stock': [], 'mean_reward': []}
    
    def reset(self):
        self.current_time = 0
        self.episode_stats = {'step': [], 'mean_effort': [], 'mean_stock': [], 'mean_reward': []}

    def compute_effort_tilde(self, agent_idx):
        """
        We compute the relative performance of all neighbors and use them in a Boltzmann 
        distribution to sample one agent with probability based on its performance.
        The temperature allows to balance exploration (trying out different agents - high T) 
        and exploitation (favoring better-performing agents - low T)
        """
        neighbors_indices = self.adjacency_matrix[agent_idx].nonzero()[0]
        agent_harvest = self.population[agent_idx].harvest
        dH_array = np.array([])
        for i in neighbors_indices:
            dH = self.population[i].harvest - agent_harvest    # evaluate neighborâ€™s success wrt the reference agent
            dH_array = np.append(dH_array, dH)
        exps = np.exp(dH_array / self.population[agent_idx].T)
        action_probs = exps / np.sum(exps)
        picked_idx = np.random.choice(neighbors_indices, p=action_probs)
      
        return self.population[picked_idx].effort_idx

    def run(self, dt=1):
        self.reset()
        states = self.get_state_indices()

        for step in range(self.num_steps):            
            # Perform actions
            for agent in self.population:
                agent.perform_action(dt) # update stock and harvest
            next_states = self.get_state_indices()
            self.current_time += dt

            # Store results
            self.stats['step'].append(step)
            self.stats['mean_effort'].append(np.mean(self.get_actions()))
            self.stats['mean_stock'].append(np.mean(self.get_stocks()))
            self.stats['mean_reward'].append(np.mean(self.get_rewards()))

            for i, agent in enumerate(self.population):
                # Update Q-tables
                agent.update_q_table(states[i], agent.effort_idx, agent.harvest, next_states[i])

                # Update actions 
                effort_tilde_index = self.compute_effort_tilde(i)
                agent.choose_action(next_states[i], effort_tilde_index)

            states = next_states

    def plot_policy_distributions(self, title):
        data = pd.DataFrame()
        for state_idx in range(len(self.y_range)):
            policies = self.get_greedy_policies(state_idx) 
            df = pd.DataFrame({
                'Action': policies,
                'State': [f'State {state_idx}'] * len(policies)
            })
            data = pd.concat([data, df], ignore_index=True)

        plt.figure(figsize=(6, 4))
        sns.violinplot(x='State', y='Action', data=data, inner='quartile', hue='State', legend=False, palette='Set2')
        plt.title(title)
        plt.xlabel('State')
        plt.ylabel('Effort Level')
        #plt.ylim(-0.02, 1.02)
        plt.show()

    def get_state_indices(self):
        return np.array([agent.resource_pool.get_state_index() for agent in self.population])

    def get_stocks(self):
        return np.array([agent.resource_pool.stock for agent in self.population])
    
    def get_rewards(self):
        return np.array([agent.harvest for agent in self.population])

    def get_actions(self):
        return np.array([agent.get_effort_value() for agent in self.population])
    
    def get_greedy_policies(self, state_index):
        return np.array([self.x_range[np.argmax(agent.q_table[state_index])] for agent in self.population]) # returns directly effort levels


#### Parameters ####

x_range = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])
y_range = np.array([0.2, 0.4, 0.6, 0.8, 1])
num_agents = 50
max_stock = 1.0
growth_rate = 1.0
alpha = 0.1
gamma = 0.99
epsilon = 0.1
T = 1
link_density = 0.35
num_steps = int(1e4)
s0_case = {"case": "constant", "value": 1.0}
#s0_case = {"case": "random", "value": np.nan}

#### Main code ####

population = [
    Agent(
        x_range, y_range, alpha, gamma, epsilon, T,
        ResourcePool(x_range, y_range, s0_case, max_stock, growth_rate)
    )
    for _ in range(num_agents)
]

adjacency_matrix = nx.adjacency_matrix(nx.erdos_renyi_graph(num_agents, link_density)).toarray()

trainer = MultiAgentTrainer(x_range, y_range, adjacency_matrix, population, num_episodes, num_steps)
trainer.run() # single episode
