# Implementation of Multi-Agent Exploitation Models with Social Learning

This repository includes four models based on the copan:EXPLOIT model, that are aiming to develop more complex learning processes. The models simulate agent behavior in a resource harvesting scenario. The agents' decision processes vary across the models, incorporating probabilistic imitation, Q-learning, and more sophisticated reinforcement learning techniques.
The original Python/Cython implementation can be found here: <https://github.com/wbarfuss/cyexploit>.

Two types of codes are given, one with explicit modules for the different parts of the model (e.g. resource pools, agents, trainer, etc.), and another that aims at improving the performance.

## Installation Guide 

<!-- Codes are written in Python. 

[Numba](https://numba.pydata.org/) and Python's [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) library are required. 
 -->
<!-- We are using [Torch](https://pytorch.org/) for the deep learning model (M-4). -->

1. Set Poetry to create virtual environments in the project directory: `poetry config virtualenvs.in-project true`
2. Clone the project repository: `git clone git@github.com:DezrannCAS/Exploit_RL.git` `cd Exploit_RL`
3. Optionally specify python version for the project: `poetry env use python3.13`
2. Install the project dependencies: `poetry install`
3. Activate the virtual environment:: `poetry shell`

## Model Description

### M-1: Probabilistic Imitation Model

This model is simply a restructuring of the original COPAN model without rewiring processes, and using Numba's Just-in-Time compilation for specific functions, intead of Cython.
Agents imitate their neighbors' harvesting decisions based on observed differences in harvest outcomes.

### M-2: Q-Learning Imitation Model

Building on M-1, this model integrates Q-learning into the agents' decision-making processes. Agents take the majority action of their neighborhood as the state, choose to imitate or not as the action, and receive rewards based on the average difference in harvest.

### M-3: Observation-Augmented Exploration Model

M-3 shifts focus from neighborhood actions to individual stock levels. 
Agents use their current stock levels as the state and decide on the effort level for harvesting as the action. 
This model combines regular RL algorithms for exploitation policies with neighbor imitation for exploration.

### M-4: Observational Learning Model

The M-4 model also uses agents' current stock levels as the state and effort levels as the action. 
However, it employs a deep learning framework combining LSTM networks, actor-critic methods, and predictors.
Importantly, in this model, all agents share a single resource pool, unlike other models in which agents learn to harvest private resources.

The implementation is inspired by those from [CleanRL](https://docs.cleanrl.dev/): [ppo_continuous_action](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_continuous_action.py) and [ppo_atari_lstm](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_atari_lstm.py).
