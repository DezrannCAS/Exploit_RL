# Implementation of Multi-Agent Exploitation Models with Social Learning

This repository includes four models based on the copan:EXPLOIT model, that are aiming to develop more complex learning processes. The models simulate agent behavior in a resource harvesting scenario. The agents' decision processes vary across the models, incorporating probabilistic imitation, Q-learning, and more sophisticated reinforcement learning techniques.
The original Python/Cython implementation can be found here: <https://github.com/wbarfuss/cyexploit>.

## Requirements 
Codes are written in Python. [Torch](https://pytorch.org/) is needed for the last model.

## Model Description

### M-1: Probabilistic Imitation Model

This model is simply a restructuring of the original COPAN model without rewiring processes, and incorporating modules for resource pools, agents, and trainer.
Agents imitate their neighbors' harvesting decisions based on observed differences in harvest outcomes.

### M-2: Q-Learning Imitation Model

Building on M-1, this model integrates Q-learning into the agents' decision-making processes. Agents take the majority action of their neighborhood as the state, choose to imitate or not as the action, and receive rewards based on the average difference in harvest.

### M-3: Observation-Augmented Exploration Model

M-3 shifts focus from neighborhood actions to individual stock levels. 
Agents use their current stock levels as the state and decide on the effort level for harvesting as the action. 
This model combines regular RL algorithms for exploitation policies with neighbor imitation for exploration.

### M-4: Observational Learning Model

The M-4 model also uses agents' current stock levels as the state and effort levels as the action. 
However, it employs a deep learning framework combining LSTM networks, actor-critic methods, and predictors.
Importantly, in this model, all agents share a single resource pool, unlike other models in which agents learn to harvest private resources.

The implementation is inspired by those from [CleanRL](https://docs.cleanrl.dev/): [ppo_continuous_action](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_continuous_action.py) and [ppo_atari_lstm](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_atari_lstm.py).
