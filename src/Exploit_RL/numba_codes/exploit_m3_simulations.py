from dataclasses import dataclass
import numpy as np
from typing import Tuple
from numba import njit, prange, int32, float64
from multiprocessing import Pool, cpu_count
from functools import partial
from itertools import product
import networkx as nx
from tqdm import tqdm
import time
import pickle


######## PARAMETERS ########

@dataclass
class Args:
    # Fixed parameters
    x_values = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])
    """discrete action values"""
    y_upbounds = np.array([0.2, 0.4, 0.6, 0.8, 1])
    """upper bounds for stock bins"""
    num_episodes: int = 1000
    """number of episodes during training"""
    num_steps: int = 100
    """number of steps per episode"""
    num_agents: int = 50
    """number of agents in interaction"""
    max_stock: float = 1.0
    """resources capacity"""
    growth_rate: float = 0.7
    """resources intrinsic growth"""
    link_density: float = 0.35
    """density of adjacency matrix"""

    # Parameters to explore (default settings)
    alpha: float = 0.1
    """learning rate (should not be zero)"""
    gamma: float = 0.99
    """discount factor (can be 0)"""
    epsilon: float = 0.1
    """exploration factor (should not be 0)"""
    T_dH: float = 1.0
    """temperature for action tilde selection (cannot be zero)"""

    # Simulation features
    file_name: str = 'resultsm3_aget.pkl'
    """path to saved results"""
    num_simulations: int = 50
    """number of simulation per datapoints, to get average results"""
    step: float = 0.1
    """grid resolution"""


######## JIT FUNCTIONS ########

@njit(parallel=True)
def sample_neighbors(adjacency_matrix, harvests, T_dH):
    num_agents = harvests.shape[0]
    picked_agent_indices = np.empty(num_agents, dtype=np.int32)

    for i in prange(num_agents):
        neighbors_indices = adjacency_matrix[i].nonzero()[0]
        dHs = harvests[neighbors_indices] - harvests[i]

        exps = np.exp(dHs / T_dH)
        action_probs = exps / np.sum(exps)

        r = np.random.rand()
        cum_probs = 0.0

        for j in range(neighbors_indices.shape[0]):
            cum_probs += action_probs[j]
            if r < cum_probs:
                picked_agent_indices[i] = neighbors_indices[j]
                break

    return picked_agent_indices

@njit(parallel=True)
def choose_actions_epsgreedy(states, effort_tilde_indices, q_tables, epsilon):
    num_agents = states.shape[0]
    actions = np.empty(num_agents, dtype=np.int32)

    for i in prange(num_agents):
        if np.random.rand() < epsilon:
            actions[i] = effort_tilde_indices[i]
        else:
            actions[i] = np.argmax(q_tables[i, states[i]])

    return actions

@njit(parallel=True)
def perform_actions(stocks: np.ndarray, efforts: np.ndarray, max_stock: float, growth_rate: float, dt: float) -> Tuple[np.ndarray, np.ndarray]:
    E = efforts
    b = growth_rate - E
    gs0 = growth_rate * stocks

    # Handle b~0 cases
    new_stocks = np.empty_like(stocks)
    for i in prange(stocks.shape[0]):
        if abs(b[i]) < 1e-6:
            new_stocks[i] = stocks[i] * max_stock / (max_stock + gs0[i] * dt)
        else:
            new_stocks[i] = stocks[i] * max_stock * b[i] / ((max_stock * b[i] - gs0[i]) * np.exp(-b[i] * dt) + gs0[i])

    return new_stocks, E * new_stocks

@njit(parallel=True)
def update_q_tables(q_tables: np.ndarray, states: np.ndarray, actions: np.ndarray, rewards: np.ndarray, next_states: np.ndarray, alpha: float, gamma: float) -> np.ndarray:
    for i in prange(q_tables.shape[0]):
        best_next_action = np.argmax(q_tables[i, next_states[i]])
        td_target = (1 - gamma) * rewards[i] + gamma * q_tables[i, next_states[i], best_next_action]
        td_error = td_target - q_tables[i, states[i], actions[i]]
        q_tables[i, states[i], actions[i]] += alpha * td_error
    return q_tables

@njit(parallel=True)
def get_states(stocks, y_upbounds):
    num_agents = stocks.shape[0]
    num_bounds = y_upbounds.shape[0]
    states = np.empty(num_agents, dtype=np.int32)

    for agent_idx in prange(num_agents):
        states[agent_idx] = num_bounds  # default to the last index
        for i in range(num_bounds):
            if y_upbounds[i] >= stocks[agent_idx]:
                states[agent_idx] = i
                break

    return states

@njit(parallel=True)
def get_greedy_policies(q_tables, states, x_values):
    num_agents = q_tables.shape[0]
    best_next_action_indices = np.empty(num_agents, dtype=np.int32)
    for i in prange(num_agents):
        best_next_action_indices[i] = np.argmax(q_tables[i, states[i]])
    return x_values[best_next_action_indices]


######## TRAINER CLASS ########

class MultiAgentTrainer:
    def __init__(self, x_values, y_upbounds, adjacency_matrix, num_steps, max_stock, growth_rate, alpha, gamma, epsilon, T_dH):
        # Multi-Agent Parameters (global)
        self.adjacency_matrix = adjacency_matrix
        self.num_agents = adjacency_matrix.shape[0]
        self.x_values = x_values
        self.y_upbounds = y_upbounds
        self.num_steps = num_steps

        # Multi-Agent Variable
        self.current_time = 0

        # Resource Pools Parameters (common)
        self.max_stock = max_stock
        self.growth_rate = growth_rate

        # Resource Pools Variable (individual)
        self.stocks = np.ones(self.num_agents)

        # Agent Parameters (common)
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.T_dH = T_dH
        assert self.T_dH > 0
        assert self.epsilon > 0
        assert self.alpha > 0

        # Agent Variables (individual)
        self.harvests = np.full(self.num_agents, None)
        self.strategies = np.random.choice(range(len(self.x_values)), size=self.num_agents)
        self.q_tables = np.zeros((self.num_agents, len(self.y_upbounds), len(self.x_values)))

    def reset_episode(self):
        self.current_time = 0
        self.stocks = np.ones(self.num_agents)
        self.harvests = np.full(self.num_agents, None)
        self.effort_indices = np.random.choice(range(len(self.x_values)), size=self.num_agents)

    def run(self, update_q=True, dt=1.0):

        # Get states (stock indices)
        states = get_states(self.stocks, self.y_upbounds)

        for _ in range(self.num_steps):
            # Perform actions (get next states and harvests)
            effort_values = self.x_values[self.effort_indices]
            self.stocks, self.harvests = perform_actions(self.stocks, effort_values, self.max_stock, self.growth_rate, dt)
            assert np.all((self.stocks >= 0) & (self.stocks <= self.max_stock)), 'stock out of bound'
            next_states = get_states(self.stocks, self.y_upbounds)
            self.current_time += dt

            # Update Q-tables
            if update_q:
                self.q_tables = update_q_tables(self.q_tables, states, self.effort_indices, self.harvests, next_states, self.alpha, self.gamma)

            # Update actions (next effort indices)
            sampled_neighbors = sample_neighbors(self.adjacency_matrix, self.harvests, self.T_dH)
            effort_tilde_indices = self.effort_indices[sampled_neighbors]
            self.effort_indices = choose_actions_epsgreedy(next_states, effort_tilde_indices, self.q_tables, self.epsilon)

            # Get to next step
            states = next_states

        # return greedy policy at final state, final stocks, and harvest
        return np.array([get_greedy_policies(self.q_tables, states, self.x_values).mean(), np.mean(self.stocks), np.mean(self.harvests)])


######## MULTIPROCESS SIMULATIONS ########

def traintest_agents(seed, alpha, gamma, epsilon, T_dH, args):
    np.random.seed(seed)

    adjacency_matrix = nx.adjacency_matrix(nx.erdos_renyi_graph(args.num_agents, args.link_density)).toarray()
    assert len(np.where(adjacency_matrix.sum(axis=1) == 0)[0]) == 0
    trainer = MultiAgentTrainer(args.x_values, args.y_upbounds, adjacency_matrix, args.num_steps,
                                args.max_stock, args.growth_rate, alpha, gamma, epsilon, T_dH)

    # Train
    for _ in range(args.num_episodes):
        trainer.reset_episode()
        _ = trainer.run()

    # Test
    return trainer.run(update_q=False)

def run_simulations(alpha, gamma, epsilon, T_dH, args):
    seeds = np.random.randint(0, 1e6, size=args.num_simulations)
    simulate_partial = partial(traintest_agents, alpha=alpha, gamma=gamma, epsilon=epsilon, T_dH=T_dH, args=args)

    print('Starting multiprocessing')
    with Pool(processes=5) as pool: # test/check best number of processes
        results = list(tqdm(pool.imap(simulate_partial, seeds), total=args.num_simulations))
    print('Done multiprocessing')

    return (alpha, gamma, epsilon, T_dH), np.mean(results, axis=0) # average across simulations


######## MAIN CODE ########

if __name__ == "__main__":
    args = Args()

    # Grid resolution
    step_scale1 = args.step
    step_scale10 = args.step * 10
    
    # Values
    alpha_values = np.arange(0, 1 + step_scale1, step_scale1, dtype=float)
    #gamma_values = np.arange(0, 1 + step_scale1, step_scale1, dtype=float)
    gamma_values = [args.gamma] # gamma fixed
    epsilon_values = np.arange(0, 1 + step_scale1, step_scale1, dtype=float)
    TdH_values = np.arange(0, 10 + step_scale10, step_scale10, dtype=float)
    
    # Non-zeros
    eps = 0.01
    alpha_values[0] += eps
    epsilon_values[0] += eps
    TdH_values[0] += eps
    
    # Resume
    print('===================================')
    num_params = len(alpha_values) * len(gamma_values) * len(epsilon_values) * len(TdH_values)
    print(f'Number of parameters: {num_params}')
    print(f'Number of simulations: {args.num_simulations}')
    print(f'--- hence {num_params * args.num_simulations} parallel processes')
    print(f'Number of agents: {args.num_agents} (most functions using parallelized loops)')
    print(f'Number of CPU logical processors: {cpu_count()}')
    print('===================================')

    # Simulate for all coordinates
    param_grid = list(product(alpha_values, gamma_values, epsilon_values, TdH_values))

    results = {}
    total_time = 0
    for i, (alpha, gamma, epsilon, T_dH) in enumerate(param_grid):
        progress = (i / num_params) * 100
        print()
        print(f"Running simulations for alpha={alpha}, gamma={gamma}, T_dH={T_dH} ({progress:.2f}% complete)")
        start_single_bsimul = time.time()
        
        params, mean_result = run_simulations(alpha, gamma, epsilon, T_dH, args)
        results[params] = mean_result
        
        time_single_bsimul = time.time() - start_single_bsimul
        total_time += time_single_bsimul
        remaining_time = (num_params-i) * total_time / (i+1)
        print(f"---simulation batch completed in {time_single_bsimul:.2f} seconds")
        print(f"---on average {total_time / (i+1):.2f} seconds, expected remaining time: {remaining_time/60:.2f} minutes")

    print()
    print(f"Completed all simulations in {total_time/60:.2f} minutes")

    with open(args.file_name, mode='wb') as file:
        pickle.dump(results, file)
    print('Results saved')
