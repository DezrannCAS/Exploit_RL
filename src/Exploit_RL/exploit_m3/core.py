import numpy as np
from typing import Tuple
from numba import njit, prange


######## JIT FUNCTIONS ########

@njit(parallel=True)
def sample_neighbors(adjacency_matrix, harvests, T_dH):
    num_agents = harvests.shape[0]
    picked_agent_indices = np.empty(num_agents, dtype=np.int32)

    for i in prange(num_agents):
        neighbors_indices = adjacency_matrix[i].nonzero()[0]
        dHs = harvests[neighbors_indices] - harvests[i]

        exps = np.exp(dHs / T_dH)
        action_probs = exps / np.sum(exps)

        r = np.random.rand()
        cum_probs = 0.0

        for j in range(neighbors_indices.shape[0]):
            cum_probs += action_probs[j]
            if r < cum_probs:
                picked_agent_indices[i] = neighbors_indices[j]
                break

    return picked_agent_indices

@njit(parallel=True)
def choose_actions_epsgreedy(states, effort_tilde_indices, q_tables, epsilon):
    num_agents = states.shape[0]
    actions = np.empty(num_agents, dtype=np.int32)

    for i in prange(num_agents):
        if np.random.rand() < epsilon:
            actions[i] = effort_tilde_indices[i]
        else:
            actions[i] = np.argmax(q_tables[i, states[i]])

    return actions

@njit(parallel=True)
def perform_actions(stocks: np.ndarray, efforts: np.ndarray, max_stock: float, growth_rate: float, dt: float) -> Tuple[np.ndarray, np.ndarray]:
    E = efforts
    b = growth_rate - E
    gs0 = growth_rate * stocks

    # Handle b~0 cases
    new_stocks = np.empty_like(stocks)
    for i in prange(stocks.shape[0]):
        if abs(b[i]) < 1e-6:
            new_stocks[i] = stocks[i] * max_stock / (max_stock + gs0[i] * dt)
        else:
            new_stocks[i] = stocks[i] * max_stock * b[i] / ((max_stock * b[i] - gs0[i]) * np.exp(-b[i] * dt) + gs0[i])

    return new_stocks, E * new_stocks

@njit(parallel=True)
def update_q_tables(q_tables: np.ndarray, states: np.ndarray, actions: np.ndarray, rewards: np.ndarray, next_states: np.ndarray, alpha: float, gamma: float) -> np.ndarray:
    for i in prange(q_tables.shape[0]):
        best_next_action = np.argmax(q_tables[i, next_states[i]])
        td_target = (1 - gamma) * rewards[i] + gamma * q_tables[i, next_states[i], best_next_action]
        td_error = td_target - q_tables[i, states[i], actions[i]]
        q_tables[i, states[i], actions[i]] += alpha * td_error
    return q_tables

@njit(parallel=True)
def get_states(stocks, y_upbounds):
    num_agents = stocks.shape[0]
    num_bounds = y_upbounds.shape[0]
    states = np.empty(num_agents, dtype=np.int32)

    for agent_idx in prange(num_agents):
        states[agent_idx] = num_bounds  # default to the last index
        for i in range(num_bounds):
            if y_upbounds[i] >= stocks[agent_idx]:
                states[agent_idx] = i
                break

    return states

@njit(parallel=True)
def get_greedy_policies(q_tables, states, x_values):
    num_agents = q_tables.shape[0]
    best_next_action_indices = np.empty(num_agents, dtype=np.int32)
    for i in prange(num_agents):
        best_next_action_indices[i] = np.argmax(q_tables[i, states[i]])
    return x_values[best_next_action_indices]


######## TRAINER CLASS ########

class MultiAgentTrainer:
    def __init__(self, x_values, y_upbounds, adjacency_matrix, num_steps, max_stock, growth_rate, alpha, gamma, epsilon, T_dH):
        # Multi-Agent Parameters (global)
        self.adjacency_matrix = adjacency_matrix
        self.num_agents = adjacency_matrix.shape[0]
        self.x_values = x_values
        self.y_upbounds = y_upbounds
        self.num_steps = num_steps

        # Multi-Agent Variable
        self.current_time = 0

        # Resource Pools Parameters (common)
        self.max_stock = max_stock
        self.growth_rate = growth_rate

        # Resource Pools Variable (individual)
        self.stocks = np.ones(self.num_agents)

        # Agent Parameters (common)
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.T_dH = T_dH
        assert self.T_dH > 0
        assert self.epsilon > 0
        assert self.alpha > 0

        # Agent Variables (individual)
        self.harvests = np.full(self.num_agents, None)
        self.strategies = np.random.choice(range(len(self.x_values)), size=self.num_agents)
        self.q_tables = np.zeros((self.num_agents, len(self.y_upbounds), len(self.x_values)))

    def reset_episode(self):
        self.current_time = 0
        self.stocks = np.ones(self.num_agents)
        self.harvests = np.full(self.num_agents, None)
        self.effort_indices = np.random.choice(range(len(self.x_values)), size=self.num_agents)

    def run(self, update_q=True, dt=1.0):

        # Get states (stock indices)
        states = get_states(self.stocks, self.y_upbounds)

        for _ in range(self.num_steps):
            # Perform actions (get next states and harvests)
            effort_values = self.x_values[self.effort_indices]
            self.stocks, self.harvests = perform_actions(self.stocks, effort_values, self.max_stock, self.growth_rate, dt)
            assert np.all((self.stocks >= 0) & (self.stocks <= self.max_stock)), 'stock out of bound'
            next_states = get_states(self.stocks, self.y_upbounds)
            self.current_time += dt

            # Update Q-tables
            if update_q:
                self.q_tables = update_q_tables(self.q_tables, states, self.effort_indices, self.harvests, next_states, self.alpha, self.gamma)

            # Update actions (next effort indices)
            sampled_neighbors = sample_neighbors(self.adjacency_matrix, self.harvests, self.T_dH)
            effort_tilde_indices = self.effort_indices[sampled_neighbors]
            self.effort_indices = choose_actions_epsgreedy(next_states, effort_tilde_indices, self.q_tables, self.epsilon)

            # Get to next step
            states = next_states

        # return greedy policy at final state, final stocks, and harvest
        return np.array([get_greedy_policies(self.q_tables, states, self.x_values).mean(), np.mean(self.stocks), np.mean(self.harvests)])
