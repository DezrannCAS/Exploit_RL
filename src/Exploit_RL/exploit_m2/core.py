import numpy as np
from typing import Tuple
from numba import njit, prange
from scipy.sparse.csgraph import connected_components
from .. import MultiAgentSystem


######## JIT FUNCTIONS ########

@njit(parallel=True)
def choose_actions_softmax(states: np.ndarray, q_tables: np.ndarray, T: float) -> np.ndarray:    
    num_agents = q_tables.shape[0]
    actions = np.empty(num_agents, dtype=np.int32)

    for i in prange(num_agents):
        exps = np.exp(q_tables[i, states[i]] / T)
        action_probs = exps / np.sum(exps)

        r = np.random.rand()
        cum_probs = 0.0

        for j in range(action_probs.shape[0]):
            cum_probs += action_probs[j]
            if r < cum_probs:
                actions[i] = j
                break
            
    return actions

@njit
def perform_actions(stocks: np.ndarray, strategies: np.ndarray, max_stock: float, growth_rate: float, dt: float) -> Tuple[np.ndarray, np.ndarray]:
    E = growth_rate * 0.5 * (3 - 2 * strategies)
    b = growth_rate - E
    gs0 = growth_rate * stocks
    new_stocks = stocks * max_stock * b / ((max_stock * b - gs0) * np.exp(-b * dt) + gs0)
    return new_stocks, E * new_stocks

@njit(parallel=True)
def update_q_tables(q_tables: np.ndarray, states: np.ndarray, actions: np.ndarray, rewards: np.ndarray, next_states: np.ndarray, alpha: float, gamma: float) -> np.ndarray:
    num_agents = q_tables.shape[0]
    for i in prange(num_agents):
        best_next_action = np.argmax(q_tables[i, next_states[i]])
        td_target = (1 - gamma) * rewards[i] + gamma * q_tables[i, next_states[i], best_next_action]
        td_error = td_target - q_tables[i, states[i], actions[i]]
        q_tables[i, states[i], actions[i]] += alpha * td_error
    return q_tables

@njit
def update_strategies(strategies: np.ndarray, actions: np.ndarray, states: np.ndarray) -> np.ndarray:
    mask = (actions == 1) # agents who imitate
    strategies[mask] = states[mask] 
    return strategies

@njit
def get_states(strategies: np.ndarray, adjacency_matrix: np.ndarray) -> np.ndarray:
    neighbors_strategies = np.dot(adjacency_matrix.astype(np.float64), strategies.astype(np.float64)) # only takes floating-point 
    majority_strategies = (neighbors_strategies >= (adjacency_matrix.sum(axis=1) / 2)).astype(np.int32)
    return majority_strategies


######## TRAINER CLASS ########

class MultiAgentTrainer(MultiAgentSystem): 
    def initialize(self, fixed_params, free_params):
        # Resource Pools Parameters (common)
        self.max_stock = fixed_params.max_stock
        self.growth_rate = fixed_params.growth_rate

        # Resource Pools Variable (individual)
        self.stocks = np.ones(self.num_agents)

        # Agent Parameters (common)
        self.alpha = free_params.alpha
        self.gamma = free_params.gamma
        self.T = free_params.T
        assert self.T > 0
        assert self.alpha > 0

        # Agent Variables (individual)
        self.harvests = np.full(self.num_agents, np.nan)
        self.strategies = np.random.randint(2, size=self.num_agents)
        self.q_tables = np.zeros((self.num_agents, 2, 2))
        
        self.check_for_consensus()

    def reset_episode(self):
        self.current_time = 0
        self.stocks = np.ones(self.num_agents)
        self.harvests = np.full(self.num_agents, np.nan)
        self.strategies = np.random.randint(2, size=self.num_agents)

    def run(self, update_q=True, max_steps=10000, dt=1.0):

        # Get states (majority strategy)
        states = get_states(self.strategies, self.adjacency_matrix)

        for _ in range(max_steps):
            # Perform actions (get harvests)
            self.stocks, self.harvests = perform_actions(self.stocks, self.strategies, self.max_stock, self.growth_rate, dt)
            self.current_time += dt

            # Update actions (imitate or not)
            actions = choose_actions_softmax(states, self.q_tables, self.T)

            # Update strategies (get next states)
            self.strategies = update_strategies(self.strategies, actions, states)
            next_states = get_states(self.strategies, self.adjacency_matrix)

            # Update Q-tables
            if update_q:
                harvest_diffs = self.harvests[:, None] - self.harvests[None, :]
                masked_diffs = np.where(self.adjacency_matrix, harvest_diffs, np.nan)
                agents_dH = np.nanmean(masked_diffs, axis=1)
                
                self.q_tables = update_q_tables(self.q_tables, states, actions, agents_dH, next_states, self.alpha, self.gamma)

            # Stop if consensus
            if self.check_for_consensus():
                return np.array([1, np.mean(self.strategies), self.current_time]) # consensus, fraction of 1s, end time

            # Get to next step
            states = next_states

        return np.array([0, np.mean(self.strategies), self.current_time]) # consensus, fraction of 1s, end time

    def check_for_consensus(self):
        cc = connected_components(self.adjacency_matrix, directed=False)[1]
        self.consensus = all(len(np.unique(self.strategies[c])) == 1
                             for c in ((cc == i).nonzero()[0]
                             for i in np.unique(cc)))
        return self.consensus
