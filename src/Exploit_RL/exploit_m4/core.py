import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import random
import os
import csv

class ResourcePool:
    def __init__(self, s0_case, max_stock, growth_rate, num_steps, dt):
        self.s0_case = s0_case
        if self.s0_case["case"] == "random":
            self.stock = random.random()
        elif self.s0_case["case"] == "constant":
            self.stock = self.s0_case["value"]
        self.max_stock = max_stock
        self.growth_rate = growth_rate
        self.dt = dt
        self.num_steps = num_steps
        self.current_time = 0

    def reset(self):
        self.current_time = 0
        if self.s0_case["case"] == "random":
            self.stock = random.random()
        elif self.s0_case["case"] == "constant":
            self.stock = self.s0_case["value"]

    def step(self, E):
        s0 = self.stock
        smax = self.max_stock
        g = self.growth_rate
        b = g - E
        gs0 = g * s0
        if abs(b) < 1e-6:
            s1 = s0 * smax / (smax + gs0 * self.dt)
        else:
            s1 = s0 * smax * b / ((smax * b - gs0) * np.exp(-b * self.dt) + gs0)
        assert s1 >= 0 and s1 <= smax, 'stock out of bound'
        harvest = E * s1
        self.stock = s1

        self.current_time += self.dt
        if self.current_time >= self.num_steps:
            done = 1
            self.reset()
        else:
            done = 0

        return harvest, done

class ReplayBuffer:
    def __init__(self, num_minibatches, len_trajectories):
        self.num_minibatches = num_minibatches
        self.len_trajectories = len_trajectories
        self.clear_memory()

    def clear_memory(self):
        self.obs = []
        self.actions = []
        self.log_probs = []
        self.values = []
        self.rewards = []
        self.dones = []
        self.lstm_h_states = []
        self.lstm_c_states = []

    def store_memory_personal(self, actions, log_probs, values, lstm_h_state, lstm_c_state):
        self.actions.append(actions)
        self.log_probs.append(log_probs)
        self.values.append(values)
        self.lstm_h_states.append(lstm_h_state)
        self.lstm_c_states.append(lstm_c_state)

    def store_memory_environment(self, obs, rewards, dones):
        self.obs.append(obs)
        self.rewards.append(rewards)
        self.dones.append(dones)

    def generate_minibatches(self):
        """
        Divides the data in trajectories of fixed size, and mini-batches with a number of trajectories.
        """
        data_size = len(self.obs)
        num_trajectories = data_size // self.len_trajectories
        num_trajectories_per_mb = num_trajectories // self.num_minibatches

        def split_into_trajectories(dataset, traj_length):
            trajectories = []
            for i in range(0, len(dataset), traj_length):
                if i + traj_length <= len(dataset):
                    trajectories.append(dataset[i:i + traj_length])
            return trajectories

        def gather_in_minibatches(trajectories, mb_indices):
            mb_trajectories = []
            for idx in mb_indices:
                traj_tensor = torch.stack(trajectories[idx])
                mb_trajectories.append(traj_tensor) # list of sequences of tensors
            return mb_trajectories

        # Split dataset into trajectories respecting temporal order (for LSTM and prediction)
        traj_obs = split_into_trajectories(self.obs, self.len_trajectories)
        traj_actions = split_into_trajectories(self.actions, self.len_trajectories)
        traj_log_probs = split_into_trajectories(self.log_probs, self.len_trajectories)
        traj_values = split_into_trajectories(self.values, self.len_trajectories)
        traj_rewards = split_into_trajectories(self.rewards, self.len_trajectories)
        traj_dones = split_into_trajectories(self.dones, self.len_trajectories)
        traj_lstm_hs = split_into_trajectories(self.lstm_h_states, self.len_trajectories)
        traj_lstm_cs = split_into_trajectories(self.lstm_c_states, self.len_trajectories)

        # Shuffle the trajectories
        traj_indices = list(range(len(traj_obs)))
        random.shuffle(traj_indices)

        assert len(self.obs) == len(self.actions) == len(self.lstm_h_states)
        assert len(traj_indices) == num_trajectories, 'incoherent number of trajectories'
        assert num_trajectories >= self.num_minibatches, 'more minibatches than trajectories'

        # Group them in mini-batch
        for i in range(0, num_trajectories, num_trajectories_per_mb):
            mb_indices = traj_indices[i:i + num_trajectories_per_mb]

            yield (gather_in_minibatches(traj_obs, mb_indices),
                  gather_in_minibatches(traj_actions, mb_indices),
                  gather_in_minibatches(traj_log_probs, mb_indices),
                  gather_in_minibatches(traj_values, mb_indices),
                  gather_in_minibatches(traj_rewards, mb_indices),
                  gather_in_minibatches(traj_dones, mb_indices),
                  gather_in_minibatches(traj_lstm_hs, mb_indices),
                  gather_in_minibatches(traj_lstm_cs, mb_indices))

    def get_memory(self):
        return (self.obs, self.actions, self.log_probs, self.values, self.rewards, self.dones)

class Network(nn.Module):
    def __init__(self, device, input_dim=1, action_dim=1):
        super(Network, self).__init__()
        self.device = device
        self.input_layer = nn.Sequential(
            nn.Linear(input_dim, 192),
            nn.Tanh()
        )
        self.lstm = nn.LSTM(192, 192)
        self.hidden_net = nn.Sequential(
            nn.Linear(192, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh()
        )
        self.alpha_layer = nn.Sequential(
            nn.Linear(64, action_dim),
            nn.Softplus()  # output > 0
        )
        self.beta_layer = nn.Sequential(
            nn.Linear(64, action_dim),
            nn.Softplus()  # output > 0
        )
        self.value_net = nn.Sequential(
            nn.Linear(192, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 1)
        )
        self.prediction_net = nn.Sequential(
            nn.Linear(input_dim + action_dim, 64),
            nn.Tanh(),
            nn.Linear(64, input_dim),
            nn.Sigmoid()  # predictions between 0 and 1
        )

    def lstm_forward(self, x, done, lstm_state):
        """
        When processing complete trajectories, the hidden and cell states need to be initialized
        for each trajectory and updated sequentially within each trajectory.
        We make sure that sequences are fed into the LSTM in a manner that respects the temporal order
        (cf. generate_minibatches) and take into account when an episode is done. 
        """
        # Reshape x to (seq_len, batch_size, input_size)
        batch_size = lstm_state[0].shape[1]
        x = x.reshape((-1, batch_size, self.lstm.input_size))
        done = done.reshape((-1, batch_size))

        # Process each step of input data
        new_x = []
        for h, d in zip(x, done):
            # Update the LSTM state based on the done flag (reset if done)
            h, lstm_state = self.lstm(
                h.unsqueeze(0),
                (
                    (1.0 - d).view(1, -1, 1) * lstm_state[0],
                    (1.0 - d).view(1, -1, 1) * lstm_state[1],
                ),
            )
            new_x += [h]
        new_x = torch.flatten(torch.cat(new_x), 0, 1)

        return new_x, lstm_state

    def forward(self, input, done, lstm_state):
        """
        Returns action, logprob, entropy, and value
        """
        input = input.to(self.device)
        done = done.to(self.device)
        lstm_state = (lstm_state[0].to(self.device), lstm_state[1].to(self.device))

        x = self.input_layer(input)
        x, lstm_state = self.lstm_forward(x, done, lstm_state)
        h = self.hidden_net(x)
        alpha = self.alpha_layer(h) + 1e-5  # adding epsilon to make sure avoiding zero
        beta = self.beta_layer(h) + 1e-5
        action_dist = torch.distributions.Beta(alpha, beta) # between 0 and 1
        action = action_dist.sample()
        logprob = action_dist.log_prob(action)
        entropy = action_dist.entropy()
        value = self.value_net(x)

        return action, logprob, entropy, value, lstm_state

    def predict(self, state, action):
        """
        Returns predicted next state given current state and agent's own action
        """
        combined_input = torch.cat([state, action], dim=1)
        next_state_prediction = self.prediction_net(combined_input)
        return next_state_prediction

    def save_checkpoint(self, model_path):
        torch.save(self.state_dict(), model_path)

    def load_checkpoint(self, model_path):
        self.load_state_dict(torch.load(model_path))


class Agent:
    def __init__(self, network, replay_buffer, learning_rate, gamma, gae_lambda, clip_coef, c_val, c_aux, c_ent):
        self.network = network.to(network.device)
        self.replay_buffer = replay_buffer
        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate, eps=1e-5)
        self.reset_lstm()

        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_coef = clip_coef

        self.c_val = c_val
        self.c_aux = c_aux
        self.c_ent = c_ent

    def reset_lstm(self, batch_size=1):
        h_0 = torch.zeros(self.network.lstm.num_layers, batch_size, self.network.lstm.hidden_size)
        c_0 = torch.zeros(self.network.lstm.num_layers, batch_size, self.network.lstm.hidden_size)
        self.lstm_state = (h_0, c_0)

    def select_action(self, state_tensor, done):
        """
        Only used to collect experience, updates the LSTM hidden state
        """
        with torch.no_grad():
            action, log_prob, _, value, self.lstm_state = self.network(state_tensor, done, self.lstm_state)
        return action, action.item(), log_prob, value

    def compute_returns_and_advantages(self, rewards, dones, values, normalize=True):
        advs = torch.zeros_like(rewards)
        last_gae = 0
        for i in reversed(range(len(rewards))):
            delta = rewards[i] + self.gamma * (values[i + 1] if i + 1 < len(values) else 0) * (1 - dones[i]) - values[i]
            last_gae = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * last_gae
            advs[i] = last_gae
        if normalize:
            advs = (advs - advs.mean()) / (advs.std() + 1e-8)
        return advs + values, advs

    def update(self):
        num_trajectories = len(self.replay_buffer.obs) // self.replay_buffer.len_trajectories
        epoch_losses = np.zeros((num_trajectories, 5))
        traj_count = 0

        # Loop over mini-batches
        for obs_batch, actions_batch, log_probs_batch, values_batch, rewards_batch, dones_batch, lstm_hidden_states_batch, lstm_cell_states_batch in self.replay_buffer.generate_minibatches():

            # Loop over trajectories
            for mb_traj_idx in range(len(obs_batch)):
                current_obs_traj = obs_batch[mb_traj_idx]
                current_actions_traj = actions_batch[mb_traj_idx]
                current_log_probs_traj = log_probs_batch[mb_traj_idx]
                current_values_traj = values_batch[mb_traj_idx]
                current_rewards_traj = rewards_batch[mb_traj_idx]
                current_dones_traj = dones_batch[mb_traj_idx]
                current_lstm_hidden_states = lstm_hidden_states_batch[mb_traj_idx]
                current_lstm_cell_states = lstm_cell_states_batch[mb_traj_idx]

                # Compute new distributions and values
                initial_lstm_state = (current_lstm_hidden_states[0], current_lstm_cell_states[0])
                _, new_log_probs, entropy, new_values, _ = self.network(current_obs_traj, current_dones_traj, initial_lstm_state)

                # Compute advantages and returns with stored rewards and values
                returns_batch, advs_batch = self.compute_returns_and_advantages(current_rewards_traj, current_dones_traj, current_values_traj)

                # Calculate entropy loss
                entropy_loss = entropy.mean()

                # Calculate policy loss (clipped surrogate objective)
                ratios = torch.exp(new_log_probs - current_log_probs_traj)
                surr1 = advs_batch * ratios
                surr2 = advs_batch * torch.clamp(ratios, 1 - self.clip_coef, 1 + self.clip_coef)
                policy_loss = -torch.min(surr1, surr2).mean()

                # Calculate value loss
                value_loss = nn.MSELoss()(new_values, returns_batch) # can also use clipped version, hence using values_batch as well

                # Calculate prediction loss
                next_states_batch = current_obs_traj[1:] # next states from obs shifted by one
                predicted_next_states = self.network.predict(current_obs_traj[:-1], current_actions_traj[:-1]) # predict with all expect last
                non_terminal_mask = (current_dones_traj[:-1] == 0) # find indices of non terminal points
                prediction_loss = nn.MSELoss()(predicted_next_states[non_terminal_mask], next_states_batch[non_terminal_mask]) # check accuracy except for new episode prediction

                # Total loss
                loss = policy_loss + self.c_val * value_loss + self.c_aux * prediction_loss - self.c_ent * entropy_loss

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                epoch_losses[traj_count] = [policy_loss.item(), value_loss.item(), prediction_loss.item(), entropy_loss.item(), loss.item()]
                traj_count += 1

            # Hidden states and advantage values can be recalculated between mini-batch updates...

        assert traj_count == num_trajectories
        mean_losses = np.mean(epoch_losses, axis=0)

        return mean_losses[0], mean_losses[1], mean_losses[2], mean_losses[3], mean_losses[4]

    def sketch_policy_profile(self, num_samples):
        numbers = [0] + [random.random() for _ in range(num_samples-2)] + [1]
        sampled_stock_levels = sorted(numbers)

        def simulate_policy(policy, stock_level):
            state_tensor = torch.tensor(stock_level, dtype=torch.float32).view(-1)
            effort_levels = []
            for _ in range(200):
                _, action, _, _ = policy.select_action(state_tensor, torch.tensor([0]))
                effort_levels.append(action)
            return np.mean(effort_levels)

        resulting_efforts = np.array([simulate_policy(self, stock_level) for stock_level in sampled_stock_levels])

        # Plot
        plt.figure(figsize=(10, 6))
        plt.plot(sampled_stock_levels, resulting_efforts, marker='o')
        plt.xlabel('Stock Level')
        plt.ylabel('Mean Effort')
        plt.title('Policy Profile')
        plt.grid(True)
        plt.show()

class MultiAgentTrainer:
    def __init__(self, shared_pool, population, num_iterations, num_episodes, num_epochs, checkpoint_dir='checkpoints'):
        self.shared_pool = shared_pool
        self.population = population
        self.num_iterations = num_iterations
        self.num_episodes = num_episodes
        self.num_epochs = num_epochs
        self.loss_dict = {
            'policy_losses': [],
            'value_losses': [],
            'prediction_losses': [],
            'entropy_losses': [],
            'total_losses': []
        }

        # Directory path where checkpoints will be saved
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(self.checkpoint_dir, exist_ok=True)

    def multiagent_collect(self, update_buffer=True, save_metrics=False):
        """
        Collect experience; function used for training (default setting) and testing (opposite booleans)
        """
        self.shared_pool.reset()
        done = 0

        if save_metrics:
            episode_metrics = []

        while not done:
            state = self.shared_pool.stock
            state_1Dtensor = torch.tensor(state, dtype=torch.float32).view(-1)

            total_action = 0

            # Select actions
            individual_efforts = []
            for agent in self.population:
                action_tensor, action, log_prob, value = agent.select_action(state_1Dtensor, torch.tensor([done]))

                total_action += action
                individual_efforts.append(action)

                if update_buffer:
                    action_1Dtensor = action_tensor.flatten()
                    logprob_1Dtensor = log_prob.flatten()
                    value_1Dtensor = value.flatten()
                    agent.replay_buffer.store_memory_personal(action_1Dtensor, logprob_1Dtensor, value_1Dtensor, agent.lstm_state[0], agent.lstm_state[1])

            # Perform total action
            total_harvest, done = self.shared_pool.step(total_action)

            if save_metrics:
                episode_metrics.append([state, total_action, total_harvest])

            if update_buffer:
                done_1Dtensor = torch.tensor(done).view(-1)
                for i, agent in enumerate(self.population):
                    if total_action != 0:
                        ind_reward = total_harvest * individual_efforts[i] / total_action
                    else:
                        ind_reward = 0
                    reward_1Dtensor = torch.tensor(ind_reward, dtype=torch.float32).view(-1)
                    agent.replay_buffer.store_memory_environment(state_1Dtensor, reward_1Dtensor, done_1Dtensor)
    
        if save_metrics:
            return episode_metrics

    def train(self):
        for iteration in range(self.num_iterations):

            # Current progress
            progress = iteration / self.num_iterations * 100
            if progress % 10 == 0 or iteration == 0 or iteration == self.num_iterations - 1:
                print(f"Progress: {progress:.0f}% complete...")

                # Save checkpoint
                if iteration != 0 and iteration != self.num_iterations - 1:
                    self.save_losses('losses.csv')
                    for j, agent in enumerate(self.population):
                        checkpoint_path = os.path.join(self.checkpoint_dir, f'checkpoint_{iteration}_agent_{j}.pth')
                        agent.network.save_checkpoint(checkpoint_path)

            # Reset agents' replay buffers and LSTM states
            for agent in self.population:
                agent.replay_buffer.clear_memory()
                agent.reset_lstm()
            
            # Collect experience
            for _ in range(self.num_episodes):
                self.multiagent_collect()

            # Update networks
            population_losses = np.zeros((len(self.population), 5))
            for _ in range(self.num_epochs):
                for i, agent in enumerate(self.population):
                    policy_loss, value_loss, prediction_loss, entropy_loss, total_loss = agent.update()
                    population_losses[i] = [policy_loss, value_loss, prediction_loss, entropy_loss, total_loss]

            mean_losses_across_agents = np.mean(population_losses, axis=0)

            # Record training losses
            self.loss_dict['policy_losses'].append(mean_losses_across_agents[0])
            self.loss_dict['value_losses'].append(mean_losses_across_agents[1])
            self.loss_dict['prediction_losses'].append(mean_losses_across_agents[2])
            self.loss_dict['entropy_losses'].append(mean_losses_across_agents[3])
            self.loss_dict['total_losses'].append(mean_losses_across_agents[4])

        # Save final losses and models
        self.save_losses('losses.csv')
        for k, agent in enumerate(self.population):
            checkpoint_path = os.path.join(self.checkpoint_dir, f'final_model_agent_{k}.pth')
            agent.network.save_checkpoint(checkpoint_path)

        return self.loss_dict

    def save_losses(self, file_name):
            with open(file_name, mode='w', newline='') as file:
                writer = csv.writer(file)
                writer.writerow(self.loss_dict.keys())
                for row in zip(*self.loss_dict.values()):
                    writer.writerow(row)

    def test(self):
        """
        Test agents' performance over one episode and plot results
        """

        def plot_sample(metrics_over_time, metric_names, title):
            num_metrics = len(metrics_over_time[0])
            num_steps = len(metrics_over_time)
            fig, axes = plt.subplots(num_metrics, 1, figsize=(8, 2 * num_metrics), sharex=True)
            for i in range(num_metrics):
                ax = axes[i]
                metric_values = [step[i] for step in metrics_over_time]
                ax.plot(range(num_steps), metric_values)
                ax.set_ylabel(metric_names[i])
            axes[-1].set_xlabel('Time steps')
            plt.suptitle(title)
            plt.tight_layout()
            plt.show()

        # Reset agents' replay buffers and LSTM states
        for agent in self.population:
            agent.replay_buffer.clear_memory()
            agent.reset_lstm()
        
        # Test and plot
        episode_metrics = self.multiagent_collect(update_buffer=False, save_metrics=True)
        plot_sample(episode_metrics, ["Stocks", "Effort", "Harvest"], f"Performance results after single-episode testing")
