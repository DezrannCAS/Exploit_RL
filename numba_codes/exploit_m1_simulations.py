from dataclasses import dataclass
import numpy as np
from typing import Tuple
from numba import njit, prange, int32, float64
from pathos.multiprocessing import Pool, cpu_count
from functools import partial
from itertools import product
import networkx as nx
from scipy.sparse.csgraph import connected_components
from tqdm import tqdm
import time
import pickle


######## PARAMETERS ########

@dataclass
class Args:
    # Fixed parameters
    num_agents: int = 50
    """number of agents in interaction"""
    max_stock: float = 1.0
    """resources capacity"""
    growth_rate: float = 0.7
    """resources intrinsic growth"""
    link_density: float = 0.35
    """density of adjacency matrix"""
    num_simulations: int = 50
    """number of simulation per datapoints, to get average results"""
    step: float = 0.1
    """grid resolution for parameters in (0,1)"""

    # Parameters to explore (default settings)
    rationality: float = 1.0
    """learning rate (can be zero)"""
    update_timescale: float = 1.0
    """discount factor (should not be zero)"""


######## JIT FUNCTIONS ########

@njit
def find_update_candidates(update_times: np.ndarray, strategies: np.ndarray, update_timescale: float, adjacency_matrix: np.ndarray) -> Tuple[int, int, float]:
    num_agents = adjacency_matrix.shape[0]
    
    for _ in range(100 * num_agents):
        # Pick the agent with the smallest update time
        agent = np.argmin(update_times)
        update_time = update_times[agent]

        # Next update time for the selected agent
        update_times[agent] += np.random.exponential(update_timescale)

        # Pick a neighbor
        neighbors = np.nonzero(adjacency_matrix[agent])[0]
        neighbor = np.random.choice(neighbors)
        
        if strategies[neighbor] != strategies[agent]:
            return agent, neighbor, update_time

    return -1, -1, 0.0

@njit
def choose_action(harvest_self: float, harvest_neighbor: float, rationality: float) -> int:    
    dH = harvest_neighbor - harvest_self
    imitation_probability = 0.5 * (np.tanh(rationality * dH) + 1)
    if np.random.rand() < imitation_probability:
        return 1 # imitate
    return 0 # not

@njit
def perform_actions(stocks: np.ndarray, strategies: np.ndarray, max_stock: float, growth_rate: float, dt: float) -> Tuple[np.ndarray, np.ndarray]:
    E = growth_rate * 0.5 * (3 - 2 * strategies)
    b = growth_rate - E
    gs0 = growth_rate * stocks
    new_stocks = stocks * max_stock * b / ((max_stock * b - gs0) * np.exp(-b * dt) + gs0)
    return new_stocks, E * new_stocks


######## TRAINER CLASS ########

class MultiAgentExploit:
    def __init__(self, adjacency_matrix, update_timescale, rationality, max_stock, growth_rate):
        # Multi-Agent Parameters (global)
        self.adjacency_matrix = adjacency_matrix
        self.num_agents = adjacency_matrix.shape[0]
        self.update_timescale = update_timescale

        # Resource Pools Parameters (common)
        self.max_stock = max_stock
        self.growth_rate = growth_rate

        # Resource Pools Variable (individual)
        self.stocks = np.ones(self.num_agents)

        # Agent Parameters
        self.rationality = rationality # (common)
        self.update_times = np.random.exponential(scale=self.update_timescale, size=self.num_agents) # (individual)

        # Agent Variables (individual)
        self.harvests = np.full(self.num_agents, None)
        self.strategies = np.random.randint(2, size=self.num_agents)
        
        # Multi-Agent Variables
        self.current_time = 0
        self.check_for_consensus()

    def reset_episode(self):
        self.current_time = 0
        self.stocks = np.ones(self.num_agents)
        self.harvests = np.full(self.num_agents, None)
        self.strategies = np.random.randint(2, size=self.num_agents)

    def run(self, max_steps=1000000000):
        
        for _ in range(max_steps):
            # Find next agent to update and pick a neighbor
            agent_index, neighbor_index, update_time = find_update_candidates(self.update_times, self.strategies, self.update_timescale, self.adjacency_matrix)
            assert agent_index != -1, 'no update candidates found'
            
            # Step all agents until update_time
            dt = update_time - self.current_time
            self.stocks, self.harvests = perform_actions(self.stocks, self.strategies, self.max_stock, self.growth_rate, dt)
            self.current_time += dt
            
            # Update agent strategy
            harvest_self = self.harvests[agent_index]
            harvest_neighbor = self.harvests[neighbor_index]
            action = choose_action(harvest_self, harvest_neighbor, self.rationality)
            if action:
                self.strategies[agent_index] = self.strategies[neighbor_index]
            
            # Stop if consensus
            if self.check_for_consensus():
                return np.array([1, np.mean(self.strategies), self.current_time]) # consensus, fraction of 1s, end time

        return np.array([0, np.mean(self.strategies), self.current_time]) # consensus, fraction of 1s, end time

    def check_for_consensus(self):
        cc = connected_components(self.adjacency_matrix, directed=False)[1]
        self.consensus = all(len(np.unique(self.strategies[c])) == 1
                             for c in ((cc == i).nonzero()[0]
                             for i in np.unique(cc)))
        return self.consensus


######## MULTIPROCESS SIMULATIONS ########

def simulate_agents(seed, update_timescale, rationality, args):
    np.random.seed(seed)
    
    adjacency_matrix = nx.adjacency_matrix(nx.erdos_renyi_graph(args.num_agents, args.link_density)).toarray()
    assert len(np.where(adjacency_matrix.sum(axis=1) == 0)[0]) == 0, 'The adjacency matrix contains isolated agents'
    
    trainer = MultiAgentExploit(adjacency_matrix, update_timescale, rationality, args.max_stock, args.growth_rate)
    
    return trainer.run()

def run_simulations(update_timescale, rationality, args):    
    seeds = np.random.randint(0, 1e6, size=args.num_simulations)
    simulate_partial = partial(simulate_agents, update_timescale=update_timescale, rationality=rationality, args=args)
    
    with Pool() as pool:
        results = list(tqdm(pool.imap(simulate_partial, seeds), total=args.num_simulations))

    return (update_timescale, rationality), np.mean(results, axis=0)


######## MAIN CODE ########

if __name__ == "__main__":
    args = Args()
    
    update_timescale_values = np.array([0.01, 0.1, 0.2, 0.5, 0.8, 1, 2, 3, 5, 10])
    rationality_values = np.array([0.0, 0.1, 0.2, 0.5, 0.8, 1, 2, 3, 5, 10])
    
    print('===================================')
    num_params = len(rationality_values) * len(update_timescale_values) 
    print(f'Number of parameters: {num_params}')
    print(f'Number of simulations: {args.num_simulations}')
    print(f'--- hence {num_params * args.num_simulations} parallel environments')
    print(f'Number of agents: {args.num_agents} (not in parallel)')
    print(f'Number of CPU logical processors: {cpu_count()}')
    print('===================================')
    
    param_grid = list(product(update_timescale_values, rationality_values))

    results = {}
    total_time = 0
    for i, (update_timescale, rationality) in enumerate(param_grid):
        progress = (i / num_params) * 100
        print()
        print(f"Running simulations for r={rationality}, tau={update_timescale} (total {progress:.2f}% complete)")
        start_single_bsimul = time.time()
        
        params, mean_result = run_simulations(update_timescale, rationality, args)
        results[params] = mean_result
        
        time_single_bsimul = time.time() - start_single_bsimul
        total_time += time_single_bsimul
        remaining_time = (num_params-i) * total_time / (i+1)
        print(f"---completed simulation batch in {time_single_bsimul:.2f} seconds")
        print(f"---on average {total_time / (i+1):.2f} seconds, expected remaining time: {remaining_time/60:.2f} minutes")

    print()
    print(f"Completed all simulations in {total_time/60:.2f} minutes")
    
    with open('resultsm1_taur.pkl', mode='wb') as file:
        pickle.dump(results, file)
    print('Results saved')
