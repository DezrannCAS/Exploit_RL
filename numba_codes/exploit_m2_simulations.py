from dataclasses import dataclass
import numpy as np
from typing import Tuple
from numba import njit, prange, int32, float64
from pathos.multiprocessing import Pool, cpu_count
from functools import partial
from itertools import product
import networkx as nx
from scipy.sparse.csgraph import connected_components
from tqdm import tqdm
import time
import pickle


######## PARAMETERS ########

@dataclass
class Args:
    # Fixed parameters
    num_episodes: int = 1000
    """number of episodes during training"""
    num_agents: int = 50
    """number of agents in interaction"""
    max_stock: float = 1.
    """resources capacity"""
    growth_rate: float = 0.7
    """resources intrinsic growth"""
    link_density: float = 0.35
    """density of adjacency matrix"""

    # Parameters to explore (default settings)
    alpha: float = 0.1
    """learning rate (should not be zero)"""
    gamma: float = 0.99
    """discount factor (can be 0)"""
    T: float = 1.
    """temperature for policy update (cannot be zero)"""

     # Simulation features
    file_name: str = 'resultsm2_agt.pkl'
    """path to saved results"""
    num_simulations: int = 50
    """number of simulation per datapoints, to get average results"""
    step: float = 0.1
    """grid resolution"""


######## JIT FUNCTIONS ########

@njit(parallel=True)
def choose_actions_softmax(states: np.ndarray, q_tables: np.ndarray, T: float) -> np.ndarray:    
    num_agents = q_tables.shape[0]
    actions = np.empty(num_agents, dtype=np.int32)

    for i in prange(num_agents):
        exps = np.exp(q_tables[i, states[i]] / T)
        action_probs = exps / np.sum(exps)

        r = np.random.rand()
        cum_probs = 0.0

        for j in range(action_probs.shape[0]):
            cum_probs += action_probs[j]
            if r < cum_probs:
                actions[i] = j
                break
            
    return actions

@njit
def perform_actions(stocks: np.ndarray, strategies: np.ndarray, max_stock: float, growth_rate: float, dt: float) -> Tuple[np.ndarray, np.ndarray]:
    E = growth_rate * 0.5 * (3 - 2 * strategies)
    b = growth_rate - E
    gs0 = growth_rate * stocks
    new_stocks = stocks * max_stock * b / ((max_stock * b - gs0) * np.exp(-b * dt) + gs0)
    return new_stocks, E * new_stocks

@njit(parallel=True)
def update_q_tables(q_tables: np.ndarray, states: np.ndarray, actions: np.ndarray, rewards: np.ndarray, next_states: np.ndarray, alpha: float, gamma: float) -> np.ndarray:
    num_agents = q_tables.shape[0]
    for i in prange(num_agents):
        best_next_action = np.argmax(q_tables[i, next_states[i]])
        td_target = (1 - gamma) * rewards[i] + gamma * q_tables[i, next_states[i], best_next_action]
        td_error = td_target - q_tables[i, states[i], actions[i]]
        q_tables[i, states[i], actions[i]] += alpha * td_error
    return q_tables

@njit
def update_strategies(strategies: np.ndarray, actions: np.ndarray) -> np.ndarray:
    strategies[:] = np.where(actions == 1, actions, strategies)
    return strategies

@njit
def get_states(strategies: np.ndarray, adjacency_matrix: np.ndarray) -> np.ndarray:
    neighbors_strategies = np.dot(adjacency_matrix.astype(np.float64), strategies.astype(np.float64)) # only takes floating-point 
    majority_strategies = (neighbors_strategies >= (adjacency_matrix.sum(axis=1) / 2)).astype(np.int32)
    return majority_strategies


######## TRAINER CLASS ########

class MultiAgentTrainer:
    def __init__(self, adjacency_matrix, max_stock, growth_rate, alpha, gamma, T):
        # Multi-Agent Parameters (global)
        self.adjacency_matrix = adjacency_matrix
        self.num_agents = adjacency_matrix.shape[0]

        # Resource Pools Parameters (common)
        self.max_stock = max_stock
        self.growth_rate = growth_rate

        # Resource Pools Variable (individual)
        self.stocks = np.ones(self.num_agents)

        # Agent Parameters (common)
        self.alpha = alpha
        self.gamma = gamma
        self.T = T
        assert self.T > 0
        assert self.alpha > 0

        # Agent Variables (individual)
        self.harvests = np.full(self.num_agents, None)
        self.strategies = np.random.randint(2, size=self.num_agents)
        self.q_tables = np.zeros((self.num_agents, 2, 2))
        
        # Multi-Agent Variables
        self.current_time = 0
        self.check_for_consensus()

    def reset_episode(self):
        self.current_time = 0
        self.stocks = np.ones(self.num_agents)
        self.harvests = np.full(self.num_agents, None)
        self.strategies = np.random.randint(2, size=self.num_agents)

    def run(self, update_q=True, max_steps=10000, dt=1.0):

        # Get states (majority strategy)
        states = get_states(self.strategies, self.adjacency_matrix)

        for _ in range(max_steps):
            # Perform actions (get harvests)
            self.stocks, self.harvests = perform_actions(self.stocks, self.strategies, self.max_stock, self.growth_rate, dt)
            self.current_time += dt

            # Update actions (imitate or not)
            actions = choose_actions_softmax(states, self.q_tables, self.T)

            # Update strategies (get next states)
            self.strategies = update_strategies(self.strategies, actions)
            next_states = get_states(self.strategies, self.adjacency_matrix)

            # Update Q-tables
            if update_q:
                harvest_diffs = self.harvests[:, None] - self.harvests[None, :]
                masked_diffs = np.where(self.adjacency_matrix, harvest_diffs, np.nan)
                agents_dH = np.nanmean(masked_diffs, axis=1)
                
                self.q_tables = update_q_tables(self.q_tables, states, actions, agents_dH, next_states, self.alpha, self.gamma)

            # Stop if consensus
            if self.check_for_consensus():
                return np.array([1, np.mean(self.strategies), self.current_time]) # consensus, fraction of 1s, end time

            # Get to next step
            states = next_states

        return np.array([0, np.mean(self.strategies), self.current_time]) # consensus, fraction of 1s, end time

    def check_for_consensus(self):
        cc = connected_components(self.adjacency_matrix, directed=False)[1]
        self.consensus = all(len(np.unique(self.strategies[c])) == 1
                             for c in ((cc == i).nonzero()[0]
                             for i in np.unique(cc)))
        return self.consensus


######## MULTIPROCESS SIMULATIONS ########

def traintest_agent(seed, alpha, gamma, T, args):
    np.random.seed(seed)
    
    adjacency_matrix = nx.adjacency_matrix(nx.erdos_renyi_graph(args.num_agents, args.link_density)).toarray()
    assert len(np.where(adjacency_matrix.sum(axis=1) == 0)[0]) == 0
    trainer = MultiAgentTrainer(adjacency_matrix, args.max_stock, args.growth_rate, alpha, gamma, T)
    
    # Train
    for _ in range(args.num_episodes):
        trainer.reset_episode()
        _ = trainer.run()

    # Test
    return trainer.run(update_q=False)

def run_simulations(alpha, gamma, T, args):    
    seeds = np.random.randint(0, 1e6, size=args.num_simulations)
    simulate_partial = partial(traintest_agent, alpha=alpha, gamma=gamma, T=T, args=args)
    
    with Pool() as pool:
        results = list(tqdm(pool.imap(simulate_partial, seeds), total=args.num_simulations))
            
    return (alpha, gamma, T), np.mean(results, axis=0)


######## MAIN CODE ########

if __name__ == "__main__":
    args = Args()

    # Grid resolution
    step_scale1 = args.step
    step_scale10 = args.step * 10
    
    # Values
    alpha_values = np.arange(0, 1 + step_scale1, step_scale1, dtype=float)
    gamma_values = np.arange(0, 1 + step_scale1, step_scale1, dtype=float)
    T_values = np.arange(0, 10 + step_scale10, step_scale10, dtype=float)

    # Non-zeros
    eps = 0.01
    alpha_values[0] += eps
    T_values[0] += eps

    # Resume
    print('===================================')
    num_params = len(alpha_values) * len(gamma_values) * len(T_values)
    print(f'Number of parameters: {num_params}')
    print(f'Number of simulations: {args.num_simulations}')
    print(f'--- hence {num_params * args.num_simulations} parallel environments')
    print(f'Number of agents: {args.num_agents} (in parallel for Q- and policy updates)')
    print(f'Number of CPU logical processors: {cpu_count()}')
    print('===================================')

    # Simulate for all coordinates
    param_grid = list(product(alpha_values, gamma_values, T_values))

    results = {}
    total_time = 0
    for i, (alpha, gamma, T) in enumerate(param_grid):
        progress = (i / num_params) * 100
        print()
        print(f"Running simulations for alpha={alpha}, gamma={gamma}, T={T} (total {progress:.2f}% complete)")
        start_single_bsimul = time.time()
        
        params, mean_result = run_simulations(alpha, gamma, T, args)
        results[params] = mean_result
        
        time_single_bsimul = time.time() - start_single_bsimul
        total_time += time_single_bsimul
        remaining_time = (num_params-i) * total_time / (i+1)
        print(f"---completed simulation batch in {time_single_bsimul:.2f} seconds")
        print(f"---on average {total_time / (i+1):.2f} seconds, expected remaining time: {remaining_time/60:.2f} minutes")

    print()
    print(f"Completed all simulations in {total_time/60:.2f} minutes")
    
    with open(args.file_name, mode='wb') as file:
        pickle.dump(results, file)
    print('Results saved')
